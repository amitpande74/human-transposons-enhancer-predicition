{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNuTM/GjzCroQo4A1RK2kJO"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2bGwTP3Tlajj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Masking, Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import GlobalMaxPooling1D\n",
        "from sklearn.model_selection import train_test_split\n",
        "from Bio import SeqIO\n",
        "\n",
        "def read_fasta_file(file_path, limit=None):\n",
        "    sequences = []\n",
        "    for record in SeqIO.parse(file_path, \"fasta\"):\n",
        "        sequences.append(str(record.seq))\n",
        "        if limit is not None and len(sequences) >= limit:\n",
        "            break\n",
        "    return sequences\n",
        "\n",
        "def one_hot_encode(sequence):\n",
        "      encoding = {\n",
        "        'A': [1, 0, 0, 0],\n",
        "        'C': [0, 1, 0, 0],\n",
        "        'G': [0, 0, 1, 0],\n",
        "        'T': [0, 0, 0, 1],\n",
        "        'N': [0.25, 0.25, 0.25, 0.25],\n",
        "        'Y': [0, 0.5, 0, 0.5],\n",
        "        'R': [0.5, 0, 0.5, 0],\n",
        "        'M': [0.5, 0.5, 0, 0],\n",
        "        'K': [0, 0, 0.5, 0.5],\n",
        "        'S': [0, 0.5, 0.5, 0],\n",
        "        'W': [0.5, 0, 0, 0.5],\n",
        "        'B': [0, 0.333, 0.333, 0.333],\n",
        "        'D': [0.333, 0, 0.333, 0.333],\n",
        "        'H': [0.333, 0.333, 0, 0.333],\n",
        "        'V': [0.333, 0.333, 0.333, 0]\n",
        "    }\n",
        "\n",
        "     #return np.array([encoding.get(base.upper(), None) for base in sequence if encoding.get(base.upper(), None) is not None])\n",
        "      return np.array([encoding.get(base.upper(), None) for base in sequence if encoding.get(base.upper(), None) is not None], dtype=np.float32)\n",
        "\n",
        "def preprocess_fasta_enhancer_sequences(file_path, limit=None):\n",
        "    sequences = read_fasta_file(file_path, limit=limit)\n",
        "    encoded_sequences = [one_hot_encode(seq) for seq in sequences]\n",
        "    return np.array(encoded_sequences)\n",
        "\n",
        "def extract_sequences(input_file, output_file, label, limit=10000):\n",
        "    with open(output_file, 'w') as outfile:\n",
        "        count = 0\n",
        "        for record in SeqIO.parse(input_file, \"fasta\"):\n",
        "            if count >= limit:\n",
        "                break\n",
        "            record.id = f\"{label}_{count}\"\n",
        "            record.description = f\"{label} sequence {count}\"\n",
        "            SeqIO.write(record, outfile, \"fasta\")\n",
        "            count += 1\n",
        "\n",
        "experimentally_derived_file = \"your experimental enhancer fasta file here\"\n",
        "transposon_derived_file = \"your human transposon fasta file here\"\n",
        "\n",
        "extract_sequences(experimentally_derived_file, extracted_sequences_file, label=\"experimentally_derived\")\n",
        "\n",
        "X_experimentally_derived = preprocess_fasta_enhancer_sequences(extracted_sequences_file)\n",
        "X_transposon_derived = preprocess_fasta_enhancer_sequences(transposon_derived_file)\n",
        "\n",
        "X = np.concatenate((X_experimentally_derived, X_transposon_derived))\n",
        "y = np.concatenate((np.ones(len(X_experimentally_derived)), np.zeros(len(X_transposon_derived))))\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "def data_generator(X, y, batch_size):\n",
        "    num_samples = len(X)\n",
        "    while True:\n",
        "        for start in range(0, num_samples, batch_size):\n",
        "            end = min(start + batch_size, num_samples)\n",
        "            X_batch = pad_sequences(X[start:end], padding='post')\n",
        "            y_batch = y[start:end]\n",
        "            yield X_batch, y_batch\n",
        "\n",
        "model = Sequential([\n",
        "    Masking(mask_value=0., input_shape=(None, 4)),\n",
        "    Conv1D(filters=32, kernel_size=3, activation='relu'),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Conv1D(filters=64, kernel_size=3, activation='relu'),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer=Adam(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "batch_size = 32\n",
        "train_generator = data_generator(X_train, y_train, batch_size)\n",
        "val_generator = data_generator(X_val, y_val, batch_size)\n",
        "\n",
        "steps_per_epoch_train = len(X_train) // batch_size\n",
        "steps_per_epoch_val = len(X_val) // batch_size\n",
        "\n",
        "history = model.fit(train_generator, epochs=50, steps_per_epoch=steps_per_epoch_train, validation_data=val_generator, validation_steps=steps_per_epoch_val)\n",
        "\n",
        "model.save(\"model.h5\")\n",
        "\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from Bio import SeqIO\n",
        "\n",
        "# Load the saved model\n",
        "model = load_model(\"model.h5\")\n",
        "\n",
        "# Read and one-hot encode unknown transposon sequences\n",
        "unknown_transposon_file = \"unknown transposons of the same sub class here\"\n",
        "X_unknown_transposon = preprocess_fasta_enhancer_sequences(unknown_transposon_file)\n",
        "sequences = read_fasta_file(unknown_transposon_file)\n",
        "\n",
        "# Pad the sequences\n",
        "X_unknown_transposon_padded = pad_sequences(X_unknown_transposon, padding='post')\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(X_unknown_transposon_padded)\n",
        "\n",
        "# Do something with the predictions, e.g., print them, save them to a file, etc.\n",
        "for i, pred in enumerate(predictions):\n",
        "    print(f\"Sequence {i}: Probability = {pred[0]}\")\n",
        "with open(\"predictions.txt\", \"w\") as f:\n",
        "    for i, pred in enumerate(predictions):\n",
        "        f.write(f\"Sequence {i}: Probability = {pred[0]}\\n\")\n",
        "threshold = 0.5\n",
        "for i, pred in enumerate(predictions):\n",
        "    if pred[0] > threshold:\n",
        "        print(f\"Sequence {i} is classified as an enhancer with probability {pred[0]}\")\n",
        "    else:\n",
        "        print(f\"Sequence {i} is classified as a non-enhancer with probability {pred[0]}\")\n",
        "# Assume 'predictions' is a list of probabilities obtained from the model\n",
        "sequence_probabilities = [(i, prob) for i, prob in enumerate(predictions)]\n",
        "\n",
        "# Sort by probability (descending order for strong enhancers)\n",
        "sorted_sequences = sorted(sequence_probabilities, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Select the top N sequences as strong enhancers (change N to your desired threshold)\n",
        "N = 10  # For example, choose the top 10 sequences\n",
        "strong_enhancers = sorted_sequences[:N]\n",
        "\n",
        "# Sort by probability (ascending order for weak enhancers)\n",
        "sorted_sequences = sorted(sequence_probabilities, key=lambda x: x[1])\n",
        "\n",
        "# Select the top N sequences as weak enhancers (change N to your desired threshold)\n",
        "N = 10  # For example, choose the top 10 sequences\n",
        "weak_enhancers = sorted_sequences[:N]\n",
        "\n",
        "# Print the strong and weak enhancers\n",
        "print(\"Strong enhancers:\", strong_enhancers)\n",
        "print(\"Weak enhancers:\", weak_enhancers)\n",
        "\n",
        "strong_enhancers = []\n",
        "weak_enhancers = []\n",
        "mimicking_enhancers = []\n",
        "\n",
        "for i, prob in enumerate(predictions):\n",
        "    if prob >= 0.9:\n",
        "        strong_enhancers.append((i, prob))\n",
        "    elif 0.5 <= prob < 0.9:\n",
        "        weak_enhancers.append((i, prob))\n",
        "    else:\n",
        "        mimicking_enhancers.append((i, prob))\n",
        "\n",
        "print(\"Strong enhancers:\", strong_enhancers)\n",
        "print(\"Weak enhancers:\", weak_enhancers)\n",
        "print(\"Mimicking enhancer functions:\", mimicking_enhancers)\n"
      ]
    }
  ]
}